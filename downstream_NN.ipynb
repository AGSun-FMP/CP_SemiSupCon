{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os.path as osp\n",
    "from sklearn.cluster import KMeans\n",
    "import sys \n",
    "import warnings\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "random.seed(4)\n",
    "torch.manual_seed(4)\n",
    "np.random.seed(4)\n",
    "b22=True\n",
    "dino=False\n",
    "\n",
    "cellprof=False\n",
    "def rowN_remover(x):\n",
    "    return x.split(\"_\")[0]\n",
    "mesh_ohe_matrix=pd.read_csv(\"bbbc22//b22mesh_ohe_matrix.csv\",index_col=0)\n",
    "bmoa_ohe_matrix=pd.read_csv(\"repurposing_hub/b22_bmoa_ohematrix_ID2Name.csv\",index_col=0,delimiter=\";\")\n",
    "btarget_ohe_matrix=pd.read_csv(\"repurposing_hub/b22_btarget_ohematrix_ID2Name.csv\",index_col=0,delimiter=\";\")\n",
    "\n",
    "emb_file=\"embeddings/hydra/allemb__unsup40_bbbc22_5CJ_con_neighaug_lr_2-5.csv\"\n",
    "if dino:\n",
    "    emb_file=\"embeddings/dino/emb_bs192_b22_5CJ_DINO+.csv\"\n",
    "    emb_file=\"embeddings/dino/emb_bs192_b22_5Ctiff_DINO+_nobrightcont.csv\"\n",
    "if cellprof:\n",
    "        emb_file=\"embeddings/cellprof/full_cellprofb22_nucnanfilt.csv\"\n",
    "res_dict={}\n",
    "if not dino and not cellprof:\n",
    "   con_emb=True\n",
    "else:\n",
    "   con_emb=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mesh\n",
      "838 30240\n",
      "74 0.45435133770594005 0.18121693121693122\n",
      "38 0.4850429521200517 0.2640542328042328\n"
     ]
    }
   ],
   "source": [
    "if not dino and not cellprof:\n",
    "    #Contrastive embeddings\n",
    "    embis=[\"embeddings/hydra/allemb_supbs40unsup40_bbbc22_5CJ_supcon_nolabANDnonans+_neighaug_lr_2-5_full_meshupd.csv\"]# SemiSupCon by default\n",
    "    #\"embeddings/hydra/allemb__unsup40_bbbc22_5CJ_con_neighaug_lr_2-5.csv\",\n",
    "else:\n",
    "    embis=[emb_file]\n",
    "for emb_file in embis:\n",
    "    if con_emb:\n",
    "            test_dfX0=pd.read_csv(emb_file,index_col=0,delimiter=\";\")\n",
    "    else:\n",
    "        if cellprof:\n",
    "            test_dfX0=pd.read_csv(emb_file,index_col=0,delimiter=\";\")\n",
    "        else:\n",
    "            test_dfX0=pd.read_csv(emb_file,header=None,index_col=0,delimiter=\";\")\n",
    "    #print(len(test_dfX0))\n",
    "    idkey=\"Molecules\"\n",
    "    aoi_list=[\"mesh\",\"bmoa\",\"btarget\"]#\n",
    "    for aoi in aoi_list:\n",
    "        if aoi==\"mesh\":\n",
    "            moi=mesh_ohe_matrix.copy()\n",
    "        if aoi==\"bmoa\":\n",
    "            moi=bmoa_ohe_matrix.copy()\n",
    "        if aoi==\"btarget\":\n",
    "            moi=btarget_ohe_matrix.copy()\n",
    "        if cellprof:\n",
    "            test_dfX0.index=test_dfX0.Image_Metadata_SOURCE_COMPOUND_NAME    \n",
    "        if b22 or b36:\n",
    "            if con_emb:\n",
    "                new_index=[rowN_remover(i) for i in test_dfX0.index]\n",
    "                test_dfX0[idkey]=new_index#dft.index\n",
    "            else:\n",
    "                test_dfX0[idkey]=test_dfX0.index\n",
    "            groups = [df for _, df in test_dfX0.groupby(idkey)]\n",
    "            random.shuffle(groups)\n",
    "            test_dfX0=pd.concat(groups).reset_index(drop=True)\n",
    "        t0=time.time()        \n",
    "        skf = GroupKFold(n_splits=5)\n",
    "        dX=test_dfX0.copy()\n",
    "        \n",
    "        results=[]\n",
    "        print(aoi)\n",
    "        FullReporter={}\n",
    "        test_dfX_2=test_dfX0.copy()\n",
    "        test_dfX_2=test_dfX_2[test_dfX_2[idkey].isin(moi.index)]\n",
    "        moi=moi[moi.index.isin(test_dfX_2[idkey])]\n",
    "        print(len(test_dfX_2[idkey].unique()),len(test_dfX_2.index))\n",
    "        dX=test_dfX_2.copy()\n",
    "        if con_emb:\n",
    "            start_feat,end_feat=0,224\n",
    "        if cellprof:\n",
    "            start_feat,end_feat=10,10+824\n",
    "        if dino:\n",
    "            start_feat,end_feat=2,-1\n",
    "        \n",
    "        for i, (train_index, test_index) in enumerate(skf.split(dX, groups=dX[idkey])):\n",
    "            trainX=dX.iloc[train_index].copy()\n",
    "            skX2=trainX.iloc[:,start_feat:end_feat]\n",
    "            skY2=moi.loc[trainX.Molecules]\n",
    "\n",
    "            testX=dX.iloc[test_index].copy()\n",
    "\n",
    "            class CustomDataset(Dataset):\n",
    "                def __init__(self, dataframe, transform=None):\n",
    "                    self.dataframe = dataframe\n",
    "                    self.transform = transform\n",
    "\n",
    "                def __len__(self):\n",
    "                    return len(self.dataframe)\n",
    "\n",
    "                def __getitem__(self, idx):\n",
    "                    sample = self.dataframe.iloc[idx]\n",
    "\n",
    "                    # Extract features and label from the sample\n",
    "                    features = sample.iloc[start_feat:end_feat].values.astype(float)  # Adjust 'your_feature_column_name'\n",
    "                    label = moi.loc[sample.Molecules].values  # Adjust 'your_label_column_name'\n",
    "\n",
    "                    # Apply transformations if specified\n",
    "                    if self.transform:\n",
    "                        features,label = self.transform(features,label)\n",
    "\n",
    "                    return features, label\n",
    "\n",
    "\n",
    "            \n",
    "            class ToTensor(object):\n",
    "                def __call__(self, features, label):\n",
    "                    # Convert features and label to PyTorch tensors\n",
    "                    features = torch.tensor(features, dtype=torch.float32)\n",
    "                    label = torch.tensor(label, dtype=torch.float32)  # Adjust dtype if needed\n",
    "\n",
    "                    return features, label\n",
    "\n",
    "            \n",
    "            transform = ToTensor()\n",
    "            train_dataset = CustomDataset(trainX,transform=transform)\n",
    "            test_dataset = CustomDataset(testX,transform=transform)\n",
    "            sample_index = 0\n",
    "            features, label = train_dataset[sample_index]\n",
    "            bs = 32\n",
    "            train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "            test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            num_feat=features.shape[0]\n",
    "            num_classes=label.shape[0]\n",
    "            class Net(nn.Module):\n",
    "                def __init__(self, input_size, hidden_size1, hidden_size2, num_classes, dropout_prob=0.5):\n",
    "                    super(Net, self).__init__()\n",
    "\n",
    "                    # 1st layer: 512 hidden units, ReLU nonlinearity, and dropout regularization with probability 0.5\n",
    "                    self.layer1 = nn.Sequential(\n",
    "                        nn.Linear(input_size, hidden_size1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dropout_prob)\n",
    "                    )\n",
    "\n",
    "                    # 2nd layer: 256 hidden units, ReLU nonlinearity, and dropout regularization with probability 0.5\n",
    "                    self.layer2 = nn.Sequential(\n",
    "                        nn.Linear(hidden_size1, hidden_size2),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dropout_prob)\n",
    "                    )\n",
    "\n",
    "                    # 3rd layer: Linear classification layer with as many output units as the number of classes\n",
    "                    self.classification_layer = nn.Linear(hidden_size2, num_classes)\n",
    "\n",
    "                def forward(self, x):\n",
    "                    x = self.layer1(x)\n",
    "                    x = self.layer2(x)\n",
    "                    x = self.classification_layer(x)\n",
    "                    return x\n",
    "\n",
    "            input_size = num_feat  \n",
    "            hidden_size1 = 512\n",
    "            hidden_size2 = 256\n",
    "            model = Net(input_size, hidden_size1, hidden_size2, num_classes)\n",
    "            save=False\n",
    "            if cellprof:\n",
    "                PATHx_save= \"models/b22/model_dsNN__\"+aoi+\"_\"+str(bs)+emb_file.split(\"/\")[-1].split(\".csv\")[0]\n",
    "            else:\n",
    "                PATHx_save=\"models/b22/model_dsNN__\"+aoi+\"_\"+str(bs)+emb_file.split(\"emb_\")[1]\n",
    "            logfile=\"logs/log_\"+str(i)+\"_\"+PATHx_save.split(\"/\")[-1][5:-4]\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=10**-2.5)\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            model.to(device)\n",
    "            def test(loader=test_loader,debug=False):\n",
    "                model.eval()\n",
    "                prec, rec,f1s=[],[],[]\n",
    "                matches=[]\n",
    "                praucs=[]\n",
    "                accs=[]\n",
    "                error = 0\n",
    "                error_acc = 0\n",
    "                threshold = 0.5\n",
    "                with torch.no_grad():\n",
    "                    for data in loader:\n",
    "                        features=data[0].cuda(device=device,non_blocking=True).float()\n",
    "                        preds=model(features).cpu()\n",
    "                        one_hot_encoding=data[1].cpu()\n",
    "                        flat_y_true = one_hot_encoding.view(-1)\n",
    "                        flat_preds = preds.view(-1)\n",
    "                        flat_preds_float=flat_preds.clone()\n",
    "                        flat_preds=flat_preds.clip(0, 1).round().int()\n",
    "                        with warnings.catch_warnings():\n",
    "                            warnings.simplefilter('ignore')\n",
    "                            prauc=sklearn.metrics.average_precision_score(flat_y_true,flat_preds_float.clip(0, 1),average=\"micro\")\n",
    "                        praucs.append(prauc)\n",
    "                        accs.append(np.mean(np.all(flat_y_true.numpy() == flat_preds.numpy(), axis=0)))\n",
    "                return np.mean(praucs),np.mean(accs)                       \n",
    "            def train(loader=train_loader,debug=False):\n",
    "                model.train()\n",
    "                loss_all = 0\n",
    "                praucs=[]\n",
    "                for data in train_loader:\n",
    "                    features=data[0].cuda(device=device,non_blocking=True).float()\n",
    "                    optimizer.zero_grad()\n",
    "                    preds=model(features)\n",
    "                    one_hot_encoding=data[1].to(device)\n",
    "                    flat_y_true = one_hot_encoding.view(-1)\n",
    "                    flat_preds = preds.view(-1)\n",
    "                    loss = criterion(flat_preds.float(), flat_y_true.float())\n",
    "                    loss.backward()\n",
    "                    loss_all += loss.item() #* data.num_graphs\n",
    "                    optimizer.step()\n",
    "                    flat_preds_float=flat_preds.clone().float().cpu().detach()\n",
    "                    flat_preds=flat_preds.clip(0, 1).round().int().cpu()\n",
    "                    flat_y_true=flat_y_true.cpu()\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.simplefilter('ignore')\n",
    "                        prauc=sklearn.metrics.average_precision_score(flat_y_true,flat_preds_float,average=\"micro\")\n",
    "                    praucs.append(prauc)\n",
    "                    if debug:\n",
    "                        break\n",
    "                return loss_all,np.mean(praucs)\n",
    "            best_score = None\n",
    "            ep=100\n",
    "            #print(logfile)\n",
    "            for epoch in range(1,ep):\n",
    "                loss=0\n",
    "                start=time.time()\n",
    "                loss,train_prauc=train()\n",
    "                prauc,accu=test(test_loader)\n",
    "                if save:\n",
    "                  torch.save(model.state_dict(), PATHx_save)\n",
    "                if best_score is None or prauc > best_score:\n",
    "                    best_score = prauc\n",
    "                    best_acc=accu\n",
    "                    top_ep=epoch\n",
    "                    if save:\n",
    "                        torch.save(model.state_dict(), PATHx_save[:-3]+\"_best\"+PATHx_save[-3:])\n",
    "                end=time.time()\n",
    "                diff_time=abs(start-end)\n",
    "                #print(diff_time)\n",
    "                with open(logfile,\"a+\") as g:\n",
    "                        g.writelines(\"Supervised loss \"+ str(loss)+\"\\n\")\n",
    "                        g.writelines(str(diff_time)+\"\\n\")\n",
    "                        g.writelines('Epoch: {:03d}, Loss: {:.7f}'.format(epoch, loss)+\"\\n\")\n",
    "                        g.writelines(str(prauc)+\" prauc \\n\")\n",
    "                        g.writelines(str(accu)+\" ACC \\n\")      \n",
    "            res_dict[logfile]=[top_ep,best_score,best_acc,logfile]\n",
    "            print(top_ep,best_score,best_acc)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt3",
   "language": "python",
   "name": "pt3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
